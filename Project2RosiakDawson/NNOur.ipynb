{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd # Pandas library\n",
    "import numpy as np # Numpy library\n",
    "import matplotlib.pyplot as plt # Matplotlib library\n",
    "import numpy.linalg as la # Linear algebra functions\n",
    "import math # Math library\n",
    "import random # Random library\n",
    "import seaborn as sns # Seaborn library\n",
    "\n",
    "crime_data = pd.read_csv('./SanFranciscoCrimeDataset/crime.csv',\n",
    "                          header=1,\n",
    "                          skipinitialspace=False,\n",
    "                          names=['IncidntNum', 'Category', 'Descript', 'DayOfWeek', 'Date', 'Time',\n",
    "                                'PdDistrict', 'Resolution', 'Address', 'X', 'Y', 'Location', 'PdId'])\n",
    "crime_data = crime_data[ ~crime_data['PdDistrict'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "A \"from-scratch\" neural network implementation for educational purposes.\n",
    "This version is a simple binary classififer. It allows the user to incrementally\n",
    "train the network, displaying the resulting decision boundary and full density.\n",
    "The training data is hard-coded, but there is also a loop to allow more\n",
    "custom training data. We initialize the weights to be drawn from a standard\n",
    "Gaussian distribution.\n",
    "\n",
    "A crude diagram of a NN with two hidden layers and 2-dimensional inputs.\n",
    "The final layer is returned as a weighted sum, passed through a final\n",
    "sigmoid function. This scalar will determine which class to assign to the\n",
    "input point.\n",
    "\n",
    "x1--O---O\n",
    "  \\ / \\ / \\ \n",
    "   x   x   O --\n",
    "  / \\ / \\ /\n",
    "x2--O---O\n",
    "\n",
    "@author: Joseph Anderson <jtanderson@salisbury.edu>\n",
    "@date:   28 May 2019\n",
    "\n",
    "Exercise 1: vectorize more of the operations, combine the input, output, and\n",
    "hidden layers into single matrices. \n",
    "Exercise 2: Adapt the model to learn more than two classes\n",
    "Exercise 3: Use \"convolutional\" or \"recurrent\" neuron architectures\n",
    "Exercise 4: Turn into a \"generative\" model, to generate typical examples\n",
    "from either of the two classes\n",
    "Exercise 5: Parallelize!\n",
    "\n",
    "\n",
    "For motivation/explanation, see, for example:\n",
    "https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/\n",
    "\"\"\"\n",
    "\n",
    "parse_crimes = list(set([i[0] for i in crime_data[['Category']].values]))\n",
    "\n",
    "\n",
    "\n",
    "# The dimensionality of the input data\n",
    "dim = 6\n",
    "\n",
    "# The number of hidden layers\n",
    "num_layers = 1\n",
    "\n",
    "# The size of each hidden layer\n",
    "layer_size = 2\n",
    "\n",
    "# The step size used in gradient descent\n",
    "rate = 0.1\n",
    "\n",
    "bias = True\n",
    "\n",
    "# add a dimension for bias\n",
    "if bias:\n",
    "    dim += 1\n",
    "\n",
    "# X holds N-by-d samples\n",
    "#   - N is number of samples\n",
    "#   - d is dimension\n",
    "X = np.empty((0,dim), float)\n",
    "\n",
    "# Y holds N labels of -1 or 1\n",
    "Y = np.array([])\n",
    "\n",
    "# input weights. Row i is the array of weights applied to x_i\n",
    "w_in = np.random.standard_normal((dim,layer_size))\n",
    "\n",
    "# \"Tensor\" (3-dim array) of hidden-layer output weights. \n",
    "# w_hidden[lay][i][j] is the weight between lay node i and lay+1 node j\n",
    "w_hidden = np.random.standard_normal((num_layers-1, layer_size, layer_size))\n",
    "\n",
    "# output weights, comes from last layer\n",
    "w_out = np.random.standard_normal((len(parse_crimes),layer_size))\n",
    "\n",
    "# Use the standard sigmoid function. Another option is arctan, etc.\n",
    "def sigmoid(arr):\n",
    "    return 1/(1+np.exp(-1*arr))\n",
    "\n",
    "# The derivative of the sigmoid function.\n",
    "# Check this by hand to see how convenient it is :)\n",
    "def sigmoid_deriv(arr):\n",
    "    return sigmoid(arr) * (1 - sigmoid(arr))\n",
    "\n",
    "# The squared error between to vectors/scalars\n",
    "def msqerr(pred, ans):\n",
    "    return np.sum((pred-ans)**2)/2\n",
    "\n",
    "def reset():\n",
    "    w_in = np.random.standard_normal((dim,layer_size))\n",
    "    w_hidden = np.random.standard_normal((num_layers-1, layer_size, layer_size))\n",
    "    w_out = np.random.standard_normal((len(parse_crimes),layer_size))\n",
    "    return (w_in, w_hidden, w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_step takes the weights of the network and an input point,\n",
    "returning the scalar output of the network, along with a matrix\n",
    "which is a record of the output of each intermediate node during\n",
    "the computation. This is needed for training and verification.\n",
    "\n",
    "Arguments:\n",
    "inw is the dim-by-h matrix of input weights to the first layer\n",
    "outw is the h-by-1 array of weights from the last hidden layer to the output node\n",
    "hiddenw is the num_layers-by-layer_size-by-layer_size matrix of weights between each layer\n",
    "    hidden[i] has the weights from i to i+1\n",
    "    hidden[i][j] is the array of weights into node j of layer i+1\n",
    "data is 1-by-dim row vector\n",
    "\n",
    "Returns:\n",
    "scalar value coming out of the output node\n",
    "outs is layers-by-layer_size to store the output of each node\n",
    "\"\"\"\n",
    "def forward_step(inw, outw, hiddenw, data):\n",
    "    outs = np.array([sigmoid(data @ inw)]) # 1-by-dim times dim-by-h\n",
    "    for i in range(1,num_layers):\n",
    "        # i-1 here because w[i] is output weights\n",
    "        # get the output of the last layer (sig of x) and weight it into this layer\n",
    "        ins = outs[-1] @ hiddenw[i-1]  # 1-by-h times h-by-h\n",
    "        outs = np.append(outs, [sigmoid(ins)], axis=0)\n",
    "\n",
    "    # last row of outs now holds the weighted output of the last hidden layer\n",
    "    ret = sigmoid(outs[-1] @ outw.T)\n",
    "    return ret, outs\n",
    "\n",
    "\"\"\"\n",
    "backprop analyzes how wrong the network was at predicting a given label,\n",
    "then uses the magnitude of the error to perform gradient descent on the\n",
    "edge weights throughout the network. Check this with the chain rule\n",
    "of the error function! It tracks the change in error with respect to weights,\n",
    "inputs, and outputs of every node in the network\n",
    "\n",
    "inw: dim-by-layer_size\n",
    "    weights of the input nodes\n",
    "outw: 1-by-layer_size\n",
    "    weights to the output node\n",
    "hiddenw: num_layers-1 x layer_size x layer_size\n",
    "    hiddenw[lay][i][j] is the weight between lay node i and lay+1 node j\n",
    "    a column is all input weights to that node\n",
    "outputs: num_layers x layer_size\n",
    "    record of every node's output from the forward pass\n",
    "pred: scalar predicted output\n",
    "data: the input data point\n",
    "label: scalar true output\n",
    "\"\"\"\n",
    "def backprop(inw, outw, hiddenw, outputs, pred, data, label):\n",
    "    print(f\"pred: {pred} | label:{label}\")\n",
    "    dEyo = pred - label # vector 1xM - 1xM (M = dim of input)\n",
    "    dExo = dEyo * sigmoid_deriv(outputs[-1] @ outw.T) # vector 1xM @ (1xL @ LxM)\n",
    "    dEwo =  dExo.T @ outputs[-1] #1xM @ 1xL\n",
    "    \n",
    "    # hidden layer derivatives setup\n",
    "    dEwh = np.zeros((num_layers-1, layer_size, layer_size))\n",
    "    dExh = np.zeros((num_layers, layer_size))\n",
    "    dEyh = np.zeros((num_layers, layer_size))\n",
    "    \n",
    "    # need to do output layer first, not a matrix product\n",
    "    dEyh[-1] = dExo @ outw  #  1xM @ MxL\n",
    "\n",
    "    for i in range(num_layers-2,-1,-1):\n",
    "        # i-1 to get the inputs to layer i\n",
    "        x = outputs[i-1] @ hiddenw[i-1] # 1-by-h times h-by-h\n",
    "        dExh[i] = dEyh[i] * sigmoid_deriv(x) # 1-by-h\n",
    "        dEwh[i] = outputs[i-1] * dExh[i]\n",
    "        if i > 0:\n",
    "            # prep the next layer\n",
    "            dEyh[i-1] = hiddenw[i] @ dExh[i].T # h-by-h times h-by-1\n",
    "\n",
    "    #dEwi = outputs[0] * dEyh[0] # take care of the input layer, again\n",
    "                                # not a matrix product\n",
    "    data = numpy.array([data])\n",
    "    dEwi = np.matlib.repmat(data.T, 1, layer_size) * np.matlib.repmat(dExh[0], dim, 1)  # dim-by-h broadcast dim-by-h\n",
    "\n",
    "\n",
    "    # adjust the hiden layer weights accoriding to the error.\n",
    "    # Check to see that this follows gradient descent!\n",
    "    hiddenw = hiddenw - rate * dEwh\n",
    "    inw = inw - rate * dEwi\n",
    "    outw = outw - rate * dEwo\n",
    "\n",
    "    # return the new weights\n",
    "    return inw, outw, hiddenw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rounds(train_x, train_y, num_rounds, w_in, w_out, w_hidden):    \n",
    "    # iterate as long as we're told\n",
    "    # For each epoch, it would be helpful to print the total \"loss\" -- the error\n",
    "    # across the whole training set.\n",
    "    # Often, one might choose a loss threshold (say, < 0.0001) and simply train until\n",
    "    # the loss is smaller\n",
    "    for i in range(1,num_rounds+1):\n",
    "        print(f\"Itteration i: {i}\")\n",
    "        # iterate each data point\n",
    "        loss = 0\n",
    "        for j in range(0,train_x.shape[0]):\n",
    "            dat = train_x[j]\n",
    "            if bias:\n",
    "                dat = np.append(train_x[j], [1])\n",
    "\n",
    "            # get the prediction for the point, using the current weights (model)\n",
    "            pred, vals = forward_step(w_in, w_out, w_hidden, dat)\n",
    "            # adjust the weights (model) to account for whether we're incorrect\n",
    "            w_in, w_out, w_hidden = backprop(w_in, w_out, w_hidden, vals, pred, dat, train_y[j])\n",
    "            loss += abs(pred - train_y[j])**2\n",
    "    print(\"Current loss: \" + str(loss))\n",
    "\n",
    "    return (w_in, w_out, w_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# crime_data = pd.read_csv('./SanFranciscoCrimeDataset/crime.csv',\n",
    "#                           header=1,\n",
    "#                           skipinitialspace=False,\n",
    "#                           names=['IncidntNum', 'Category', 'Descript', 'DayOfWeek', 'Date', 'Time',\n",
    "#                                 'PdDistrict', 'Resolution', 'Address', 'X', 'Y', 'Location', 'PdId'])\n",
    "# crime_data = crime_data[ ~crime_data['PdDistrict'].isna() ]\n",
    "\n",
    "# #Time\n",
    "# crime_data_timeMin = [((int(i.split(':')[0])*60) + int(i.split(':')[1])) for i in crime_data['Time']]\n",
    "# crime_data['Time in Min'] = crime_data_timeMin\n",
    "\n",
    "# #Date\n",
    "# crime_data_date = [i.split()[0] for i in crime_data['Date']]\n",
    "# crime_data_dateDays = [ ((int(i.split('/')[0])-1)*30.4167) + (int(i.split('/')[1])) for i in crime_data_date]\n",
    "# crime_data['Date in Days'] = crime_data_dateDays\n",
    "\n",
    "# #PD District\n",
    "# District_Parse = list(set([i[0] for i in crime_data[['PdDistrict']].values]))\n",
    "# crime_district_mapping = {crime: ind for ind, crime in enumerate(District_Parse)}\n",
    "# crime_data_District = [crime_district_mapping[row[1][0]] for row in crime_data[['PdDistrict']].iterrows()]\n",
    "\n",
    "# crime_data['One Hot Encoding PdDist'] = crime_data_District\n",
    "\n",
    "# #Day of week\n",
    "# Day_Parse = list(set([i[0] for i in crime_data[['DayOfWeek']].values]))\n",
    "# crime_days_mapping = {crime: ind for ind, crime in enumerate(Day_Parse)}\n",
    "# crime_data_Days = [crime_days_mapping[row[1][0]] for row in crime_data[['DayOfWeek']].iterrows()]\n",
    "\n",
    "# crime_data['One Hot Encoding DayOfWeek'] = crime_data_Days\n",
    "\n",
    "\n",
    "\n",
    "# crime_mapping = {crime: ind for ind, crime in enumerate(parse_crimes)}\n",
    "# Y = [crime_mapping[row[1][0]] for row in crime_data[['Category']].iterrows()]\n",
    "\n",
    "\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Time\n",
      "Done Date\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 10, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'One Hot Encoding PdDist'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   4242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4243\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4244\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'One Hot Encoding PdDist'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9d71ea3a4c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrime_data_District\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcrime_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'One Hot Encoding PdDist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrime_data_District\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done PD District\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4245\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4247\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4346\u001b[0m         block = make_block(values=value, ndim=self.ndim,\n\u001b[0;32m-> 4347\u001b[0;31m                            placement=slice(loc, loc + 1))\n\u001b[0m\u001b[1;32m   4348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    123\u001b[0m             raise ValueError(\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m'Wrong number of items passed {val}, placement implies '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 10, placement implies 1"
     ]
    }
   ],
   "source": [
    "crime_data = pd.read_csv('./SanFranciscoCrimeDataset/crime.csv',\n",
    "                          header=1,\n",
    "                          skipinitialspace=False,\n",
    "                          names=['IncidntNum', 'Category', 'Descript', 'DayOfWeek', 'Date', 'Time',\n",
    "                                'PdDistrict', 'Resolution', 'Address', 'X', 'Y', 'Location', 'PdId'])\n",
    "crime_data = crime_data[ ~crime_data['PdDistrict'].isna() ]\n",
    "\n",
    "#Time\n",
    "crime_data_timeMin = [((int(i.split(':')[0])*60) + int(i.split(':')[1])) for i in crime_data['Time']]\n",
    "crime_data['Time in Min'] = crime_data_timeMin\n",
    "print(\"Done Time\")\n",
    "\n",
    "#Date\n",
    "crime_data_date = [i.split()[0] for i in crime_data['Date']]\n",
    "crime_data_dateDays = [ ((int(i.split('/')[0])-1)*30.4167) + (int(i.split('/')[1])) for i in crime_data_date]\n",
    "crime_data['Date in Days'] = crime_data_dateDays\n",
    "print(\"Done Date\")\n",
    "\n",
    "#PD District\n",
    "District_Parse = list(set([i[0] for i in crime_data[['PdDistrict']].values]))\n",
    "crime_data_District = np.array([np.array([0 if District_Parse.index(d)!=i else\\\n",
    " 1 for (i,p) in enumerate(District_Parse)]) for d in\\\n",
    "crime_data[['PdDistrict']].values])\n",
    "\n",
    "print(crime_data_District)\n",
    "\n",
    "crime_data['One Hot Encoding PdDist'] = crime_data_District\n",
    "print(\"Done PD District\")\n",
    "\n",
    "#Day of week\n",
    "# Day_Parse = list(set([i[0] for i in crime_data[['DayOfWeek']].values]))\n",
    "# crime_data_Day = np.array([[0 if Day_Parse.index(d)!=i else 1 for\\\n",
    "# (i,p) in enumerate(Day_Parse)] for d in crime_data[['DayOfWeek']]\\\n",
    "# .values])\n",
    "\n",
    "# crime_data['One Hot Encoding DayOfWeek'] = crime_data_Day\n",
    "\n",
    "# print(\"Done Day of Week\")\n",
    "\n",
    "# #Category\n",
    "# Category_Parse = list(set([i[0] for i in crime_data[['Category']].values]))\n",
    "# crime_data_Category = np.array([[0 if Category_Parse.index(d)!=i\\\n",
    "# else 1 for (i,p) in enumerate(Category_Parse)] for d in\\\n",
    "# crime_data[['Category']].values])\n",
    "\n",
    "# crime_data['One Hot Encoding Category'] = crime_data_Category\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       "       ..., \n",
       "       [ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [ list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data, linearly separable classes\n",
    "# Even for this setup, the network can have a tough time getting a good model!\n",
    "# Sometimes you can even hit a \"local\" minimum where more training doesn't help,\n",
    "# we need to perturb things a bit or get more data.\n",
    "tmp = crime_data[['X', 'Y', 'Time in Min', 'Date in Days', 'One Hot Encoding PdDist', 'One Hot Encoding DayOfWeek']].values\n",
    "X = np.concatenate((crime_data['X'].values, crime_data['Y'].values,\n",
    "              crime_data['Time in Min'].values,\n",
    "              crime_data['Date in Days'].values,\n",
    "              crime_data['One Hot Encoding PdDist'].values,\n",
    "              crime_data['One Hot Encoding DayOfWeek'].values), axis=1)\n",
    "\n",
    "Y = crime_data[['One Hot Encoding Category']].values\n",
    "epochs = 1000\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34c16d4e4163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain_NN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_NN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_NN1_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_NN1_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrime_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_NN1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtrain_NN_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_NN1_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def split_sets(dataframe, train_p):\n",
    "    \"\"\"\n",
    "    Returns 2 lists of the form needed to use the KNN class using the San Francisco Crime dataset\n",
    "    -- The function uses x and y coordinate data for graphing and the type of crime as our labels.\n",
    "    **The dataframe passed can be subset to go by location to see how KNN works on each \"area\"\n",
    "    \"\"\"\n",
    "    assert(train_p < 1) # Make sure that p is a percentage\n",
    "    assert(train_p > 0) # Make sure that p is non-negative\n",
    "    \n",
    "    # Split dataframe into training and test/prediction sets\n",
    "    train_count = int(dataframe.shape[0] * train_p)\n",
    "    predict_count = int(dataframe.shape[0] * (1 - train_p))\n",
    "    train_count_Y = int(dataframe.shape[0] * train_p)\n",
    "    predict_count_Y = int(dataframe.shape[0] * (1 - train_p))\n",
    "    \n",
    "    \n",
    "    tmp_list = dataframe.values[:]\n",
    "    np.random.shuffle(tmp_list) # Shuffle the rows\n",
    "    train_list = tmp_list[0:train_count] # Take the first train_p percentage for the training data\n",
    "    predict_list = tmp_list[train_count:] # The rest go to predict data\n",
    "    \n",
    "\n",
    "    rtn_train = [[i[9], i[10], i[13], i[14], i[15], i[16]] for i in train_list]\n",
    "    rtn_predict = [[i[9], i[10], i[13], i[14], i[15], i[16]]for i in predict_list]\n",
    "    \n",
    "    rtn_train_Y = [[i[1]] for i in train_list]\n",
    "    rtn_predict_Y = [[i[1]]for i in predict_list]\n",
    "    \n",
    "    return rtn_train, rtn_predict, rtn_train_Y, rtn_predict_Y\n",
    "\n",
    "\n",
    "train_NN1, predict_NN1, train_NN1_Y, predict_NN1_Y = split_sets(crime_data, .3)\n",
    "\n",
    "train_NN = np.array(train_NN1)\n",
    "train_NN_Y = np.array(train_NN1_Y)\n",
    "\n",
    "print(train_NN)\n",
    "print(train_NN_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in, w_hidden, w_out = reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, vals = forward_step(w_in, w_out, w_hidden, dat)\n",
    "#pred : 1xM\n",
    "#vals : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in, w_out, w_hidden = train_rounds(train_NN, train_NN_Y, 1, w_in, w_out, w_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in, w_out, w_hidden = train_rounds(X, Y, 10, w_in, w_out, w_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
